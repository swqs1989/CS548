{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, LancasterStemmer\n",
    "nltk.data.path.append(\"/Users/youqiao/workspace/env/nltk_data\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv('Questions.csv',encoding='latin1')\n",
    "questions[\"content\"] = questions[\"Title\"] + \" \" + questions[\"Body\"]\n",
    "tags = pd.read_csv('Tags.csv', encoding='latin1')\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def cleantags(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def lemmatize_tokens(tokens, lemmatizer):\n",
    "    lemmatized = []\n",
    "    for item in tokens:\n",
    "        lemmatized.append(lemmatizer.lemmatize(item, pos='v'))\n",
    "    return lemmatized\n",
    "\n",
    "def extractwords(tokens):\n",
    "    new_tokens = []\n",
    "    for word, pos in nltk.pos_tag(tokens):\n",
    "        if pos[:2] == 'NN' or pos[:2] == \"JJ\":\n",
    "            new_tokens.append(word)\n",
    "    return new_tokens\n",
    "\n",
    "def excludestopwords(tokens):\n",
    "    new_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stopwords:\n",
    "            new_tokens.append(word)\n",
    "    return new_tokens\n",
    "\n",
    "def tokenize_language(text):\n",
    "    text = cleantags(text)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    no_punctuation = text.translate(translator)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "def tokenize(text):\n",
    "    text = cleantags(text)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    no_punctuation = text.translate(translator)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    lemmatizes = lemmatize_tokens(tokens, lemmatizer)\n",
    "    return lemmatizes\n",
    "\n",
    "def tokenizewithnostem(text):\n",
    "    text = cleantags(text)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    no_punctuation = text.translate(translator)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    lemmatizes = lemmatize_tokens(tokens, lemmatizer)\n",
    "    return lemmatizes\n",
    "\n",
    "def genitemset(row, wordbags):\n",
    "    itemsets = []\n",
    "    for word in wordbags:\n",
    "        if row[word] == 1:\n",
    "            itemsets.append(word)\n",
    "    return \",\".join(itemsets)\n",
    "\n",
    "\n",
    "class AssociationRule():\n",
    "    def __init__(self, wordbag=None):\n",
    "        self.wordbag = [] if wordbag is None else wordbag\n",
    "        self.df = questions\n",
    "        self.df_tfidf = None\n",
    "        self.vector = None\n",
    "\n",
    "    def preprocess(self, tag):\n",
    "        \"\"\"\n",
    "        :param tag: str\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ids_tag = tags[tags[\"Tag\"] == tag][\"Id\"]\n",
    "        self.df = pd.DataFrame(ids_tag)\n",
    "        self.df = self.df.set_index('Id').join(questions.set_index('Id'))\n",
    "        print(self.df.shape)\n",
    "\n",
    "    def tokenizing(self, ngram=1, min_df=0.1, max_df=0.9, tokenizef=None, voc=None):\n",
    "        tfidfvector = TfidfVectorizer(tokenizer=tokenizef, ngram_range=(1, ngram), min_df=min_df, max_df=max_df, stop_words=stop, vocabulary=voc)\n",
    "        text_tfidf = tfidfvector.fit_transform(self.df[\"content\"])\n",
    "        print(text_tfidf.shape)\n",
    "        self.wordbag = tfidfvector.get_feature_names()\n",
    "        self.df_tfidf = text_tfidf.toarray()\n",
    "        self.df_tfidf = pd.DataFrame(self.df_tfidf, columns=self.wordbag)\n",
    "\n",
    "        for word in self.wordbag:\n",
    "            self.df_tfidf.ix[self.df_tfidf[word] == 0, word] = 0\n",
    "            self.df_tfidf.ix[self.df_tfidf[word] != 0, word] = 1\n",
    "            self.df_tfidf[word] = self.df_tfidf[word].astype(int)\n",
    "\n",
    "    def genitemset(self):\n",
    "        self.df[\"itemset\"] = self.df.apply(lambda row: genitemset(row, self.wordbag), axis=1)\n",
    "        self.df[\"itemset\"].to_csv(\"stackoverflow.basket\", header=False, index=False)\n",
    "\n",
    "    def genarff(self):\n",
    "        self.df_tfidf.to_csv(\"stackoverflow.csv\", header=False, index=False)\n",
    "\n",
    "        arfffile = open(\"stackoverflow.arff\", \"w\")\n",
    "        arfffile.write(\"@relation stackoverflow.data\\n\\n\")\n",
    "        for s in self.wordbag:\n",
    "            s1 = s.replace(\" \", \"_\")\n",
    "            arfffile.write(\"@attribute \" + s1 + \" {0, 1}\\n\")\n",
    "        arfffile.write(\"\\n\")\n",
    "        arfffile.write(\"@data\\n\")\n",
    "        arfffile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar1 = AssociationRule(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62818, 6)\n"
     ]
    }
   ],
   "source": [
    "ar1.preprocess(\"django\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62818, 82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youqiao/env/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:93: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "ar1.tokenizing(ngram=1, min_df=0.1, max_df=0.9, tokenizef=tokenize, voc=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar1.genarff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26854, 6)\n",
      "(26854, 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youqiao/env/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:93: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "ar2 = AssociationRule(tokenize)\n",
    "ar2.preprocess(\"pandas\")\n",
    "ar2.tokenizing(ngram=1, min_df=0.1, max_df=0.9, tokenizef=tokenize, voc=None)\n",
    "ar2.genarff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of extracting those question with tag of django"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62818, 6)\n",
      "(62818, 82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youqiao/env/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:93: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "ar3 = AssociationRule(tokenize)\n",
    "ar3.preprocess(\"django\")\n",
    "ar3.tokenizing(ngram=1, min_df=0.1, max_df=0.99, tokenizef=tokenize, voc=None)\n",
    "ar3.genarff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of using given dictionary to convert text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(607282, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youqiao/env/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:92: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "languages = {\"javascript\": 0,\"java\": 1,\n",
    "             \"php\": 2,\"css\": 3,\"ruby\": 4,\n",
    "             \"c\": 5,\"swift\": 6,\"scala\": 7,\n",
    "             \"r\": 8,\"matlab\": 9,\"python\": 10}\n",
    "ar4 = AssociationRule()\n",
    "ar4.tokenizing(ngram=1, min_df=0.1, max_df=0.99, tokenizef=tokenize_language, voc=languages)\n",
    "ar4.genarff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
